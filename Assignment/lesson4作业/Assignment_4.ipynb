{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经元需要依据原始数据集训练权重、误差，依据y=wTx+b,循环迭代后返回预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用线性激活函数，就只是在y=wT+b上再加上一层线性关系，\n",
    "等于还是线性的，没有体现出隐藏层的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果以0初始化所有参数，反向传播时计算出来的每两层节点之间权重就是一样的，导致无法收敛，然而每一层中的不同的节点与相邻层的节点间的权重应该是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Softmax(x):\n",
    "    e=np.exp(x)\n",
    "    p=e / np.sum(e)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()\n",
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)#将i分为2行，第一行有5个元素\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)#灰度图\n",
    "    #负责对图像进行处理,并显示其格式,但是不能显示。\n",
    "    plt.text(1,10,str(digits.target[i-1]))#1,10表示文字说明的位置\n",
    "    plt.xticks([])#设置横轴刻度为空\n",
    "    plt.yticks([])#设置纵轴刻度为空\n",
    "plt.show()#其后跟着plt.show()才能显示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "#将矩阵按比例划分为训练子集和测试子集，并返回划分好的训练集测试集样本和训练集测试集标签\n",
    "#digits.data被划分的样本特征集；digits.target被划分的样本标签；test_size样本即测试即占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据样本总个数：(1797, 64)\n",
      "训练集数据样本个数：(1347, 64)\n",
      "测试集数据样本个数：(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(\"原始数据样本总个数：{}\".format(digits.data.shape))\n",
    "print(\"训练集数据样本个数：{}\".format(X_train.shape))\n",
    "print(\"测试集数据样本个数：{}\".format(X_test.shape))\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1/(1+np.exp(-1*z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [0.]]), 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initialized scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.zeros((dim, 1))#dim表示数组w维度，每个小数组中1个元素\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))#判断b是否是小数或整数\n",
    "    \n",
    "    return w,b\n",
    "initialize_parameters(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    #print(m)\n",
    "    A = sigmoid(np.dot(w.T,X.T)+b)\n",
    "    #print(A[0])\n",
    "    #print(i)\n",
    "    #print(Y)\n",
    "    #print(range(len(Y)))\n",
    "    #print(A[0][i])\n",
    "    #sumc=0\n",
    "    #for i in range (len(Y)):\n",
    "       # sumc += sum([ Y[i]*np.log(A[0][i]) + (1-Y[i])*np.log(1-A[0][i])])\n",
    "    #cost = -1/m * sumi\n",
    "    cost = -1/m * sum([ Y[i]*np.log(A[0][i]) + (1-Y[i])*np.log(1-A[0][i]) for i in range (len(Y))])\n",
    "    dw = 1/m * np.dot(X.T,(A.T - np.array(Y).reshape((len(Y),1)) )) #交叉熵函数对w求导\n",
    "    db = sum([1/m *(A[0][i] - Y[i]) for i in range(len(Y))]) #cost对b求导\n",
    "      \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost) #从数组的形状中删除单维度条目，即把shape中为1的维度去掉\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost\n",
    "#aa=np.random.randn(2,2)\n",
    "#print(aa)\n",
    "#print(aa.T[0])\n",
    "#bb=np.random.randn(2,2)\n",
    "#propagate(aa,bb,aa,bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38111002  0.490173  ]\n",
      " [-0.07431184  1.81532234]]\n",
      "2.6122935168249146\n"
     ]
    }
   ],
   "source": [
    "A=Y=np.random.randn(2,2)\n",
    "print(A)\n",
    "sumi=0\n",
    "for i in range (len(Y)):\n",
    "    \n",
    "    sumi+=sum(A[i])\n",
    "print(sumi)\n",
    "#cost = -1/2 * sum([ Y[i]*np.log(A[0][i]) + (1-Y[i])*np.log(1-A[0][i])])\n",
    "#print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    cost = 0.\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w,b,X_train,y_train)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - dw * learning_rate\n",
    "        b = b - db * learning_rate\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X.T) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if(A[0][i] >= 0.5):\n",
    "             Y_prediction[0][i] = 1\n",
    "        else:\n",
    "             Y_prediction[0][i] = 0\n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "\n",
    "    w, b = initialize_parameters(X_train.shape[1])\n",
    "    #print(w)\n",
    "    #print(b)\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=print_cost)\n",
    "    Y_train_prediction = predict(params['w'],params['b'],X_train)\n",
    "    Y_test_prediction = predict(params['w'],params['b'],X_test)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(Y_test)):\n",
    "        if Y_test_prediction[0][i] == Y_test[i]:\n",
    "            count += 1\n",
    "    \n",
    "\n",
    "    d['w'] = params['w']\n",
    "    d['b'] = params['b']\n",
    "    d['train_accuracy'] = 1 - np.mean(np.abs(Y_train_prediction - np.array(Y_train).reshape(1, len(Y_train))))\n",
    "    d['test_accuracy1'] = count / len(Y_test)\n",
    "    d['test_accuracy'] = 1 - np.mean(np.abs(Y_test_prediction - np.array(Y_test).reshape(1, len(Y_test))))\n",
    "    d['cost'] = costs\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.380068\n",
      "Cost after iteration 200: 0.290327\n",
      "Cost after iteration 300: 0.282879\n",
      "Cost after iteration 400: 0.276831\n",
      "{'w': array([[ 0.00000000e+00],\n",
      "       [-3.65364760e-02],\n",
      "       [ 2.49682855e-02],\n",
      "       [-9.60837037e-03],\n",
      "       [ 7.52403921e-02],\n",
      "       [ 1.21464614e-01],\n",
      "       [ 1.30135837e-01],\n",
      "       [-7.91700177e-03],\n",
      "       [ 4.87310174e-03],\n",
      "       [-1.35471961e-01],\n",
      "       [ 1.51146640e-01],\n",
      "       [ 8.45878306e-02],\n",
      "       [-5.42991896e-02],\n",
      "       [-7.22582547e-02],\n",
      "       [ 5.63660159e-03],\n",
      "       [ 1.00389844e-02],\n",
      "       [-6.68764268e-03],\n",
      "       [ 1.26927966e-02],\n",
      "       [ 1.47665929e-01],\n",
      "       [-2.67578897e-02],\n",
      "       [-2.19881563e-01],\n",
      "       [-4.03173175e-02],\n",
      "       [-1.07095679e-01],\n",
      "       [-1.19193758e-02],\n",
      "       [-4.15550491e-03],\n",
      "       [-1.76994952e-01],\n",
      "       [ 5.85474635e-02],\n",
      "       [ 1.80623683e-01],\n",
      "       [ 1.90903851e-02],\n",
      "       [ 1.57592188e-01],\n",
      "       [-1.51615282e-01],\n",
      "       [-4.79530747e-03],\n",
      "       [ 0.00000000e+00],\n",
      "       [-2.69012120e-01],\n",
      "       [-9.70964928e-03],\n",
      "       [ 2.04078948e-01],\n",
      "       [-1.02205017e-01],\n",
      "       [-2.49604520e-02],\n",
      "       [ 3.24579919e-02],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.83183328e-03],\n",
      "       [-3.43628822e-02],\n",
      "       [ 6.04577887e-02],\n",
      "       [-4.07055966e-02],\n",
      "       [ 1.19912271e-01],\n",
      "       [ 6.81723641e-02],\n",
      "       [ 9.71050226e-02],\n",
      "       [ 1.01032064e-02],\n",
      "       [-1.00209454e-03],\n",
      "       [-8.85809208e-02],\n",
      "       [ 3.01811628e-02],\n",
      "       [-8.24264294e-02],\n",
      "       [-3.22873536e-01],\n",
      "       [-4.33794225e-02],\n",
      "       [ 6.79914831e-02],\n",
      "       [-1.67254664e-02],\n",
      "       [-1.42620778e-04],\n",
      "       [-3.95675879e-02],\n",
      "       [ 3.67211910e-02],\n",
      "       [-1.08731924e-01],\n",
      "       [-1.84833080e-01],\n",
      "       [-4.35154738e-02],\n",
      "       [-9.00122933e-02],\n",
      "       [-9.27607344e-02]]), 'b': -0.006156813098669991, 'train_accuracy': 0.8938381588715665, 'test_accuracy1': 0.8977777777777778, 'test_accuracy': 0.8977777777777778, 'cost': [0.6931471805599213, 0.38006809934781355, 0.29032716025301286, 0.2828791971727892, 0.27683124686278326]}\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 500, learning_rate = 1e-2,print_cost = True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
